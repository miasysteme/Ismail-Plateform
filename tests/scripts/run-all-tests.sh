#!/bin/bash

# Script pour ex√©cuter tous les tests de la plateforme ISMAIL
# Tests unitaires, d'int√©gration et de performance

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../.." && pwd)"
TESTS_DIR="${PROJECT_ROOT}/tests"

# Couleurs pour les logs
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Fonctions utilitaires
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Variables globales
ENVIRONMENT="${ENVIRONMENT:-dev}"
BASE_URL="${BASE_URL:-http://localhost:8080}"
SKIP_UNIT_TESTS="${SKIP_UNIT_TESTS:-false}"
SKIP_INTEGRATION_TESTS="${SKIP_INTEGRATION_TESTS:-false}"
SKIP_PERFORMANCE_TESTS="${SKIP_PERFORMANCE_TESTS:-false}"
PERFORMANCE_DURATION="${PERFORMANCE_DURATION:-5m}"

# Fonction d'aide
show_help() {
    cat << EOF
Usage: $0 [OPTIONS]

Options:
    -h, --help                  Afficher cette aide
    -e, --environment ENV       Environnement (dev, staging, prod) [default: dev]
    -u, --base-url URL         URL de base des services [default: http://localhost:8080]
    -d, --duration DURATION    Dur√©e des tests de performance [default: 5m]
    --skip-unit                Ignorer les tests unitaires
    --skip-integration         Ignorer les tests d'int√©gration
    --skip-performance         Ignorer les tests de performance

Variables d'environnement:
    ENVIRONMENT               Environnement de test
    BASE_URL                  URL de base des services
    SKIP_UNIT_TESTS          Ignorer les tests unitaires (true/false)
    SKIP_INTEGRATION_TESTS   Ignorer les tests d'int√©gration (true/false)
    SKIP_PERFORMANCE_TESTS   Ignorer les tests de performance (true/false)
    PERFORMANCE_DURATION     Dur√©e des tests de performance

Exemples:
    $0                                    # Tous les tests en local
    $0 -e staging -u https://staging.ismail-platform.com
    $0 --skip-performance                 # Sans tests de performance
    $0 -d 10m                            # Tests de performance de 10 minutes

EOF
}

# Analyser les arguments
parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_help
                exit 0
                ;;
            -e|--environment)
                ENVIRONMENT="$2"
                shift 2
                ;;
            -u|--base-url)
                BASE_URL="$2"
                shift 2
                ;;
            -d|--duration)
                PERFORMANCE_DURATION="$2"
                shift 2
                ;;
            --skip-unit)
                SKIP_UNIT_TESTS="true"
                shift
                ;;
            --skip-integration)
                SKIP_INTEGRATION_TESTS="true"
                shift
                ;;
            --skip-performance)
                SKIP_PERFORMANCE_TESTS="true"
                shift
                ;;
            *)
                log_error "Option inconnue: $1"
                show_help
                exit 1
                ;;
        esac
    done
}

# V√©rifier les pr√©requis
check_prerequisites() {
    log_info "V√©rification des pr√©requis..."
    
    # V√©rifier Java pour les tests unitaires et d'int√©gration
    if [[ "$SKIP_UNIT_TESTS" != "true" || "$SKIP_INTEGRATION_TESTS" != "true" ]]; then
        if ! command -v java &> /dev/null; then
            log_error "Java n'est pas install√©"
            exit 1
        fi
        
        if ! command -v mvn &> /dev/null; then
            log_error "Maven n'est pas install√©"
            exit 1
        fi
    fi
    
    # V√©rifier K6 pour les tests de performance
    if [[ "$SKIP_PERFORMANCE_TESTS" != "true" ]]; then
        if ! command -v k6 &> /dev/null; then
            log_error "K6 n'est pas install√©"
            log_info "Installation: https://k6.io/docs/getting-started/installation/"
            exit 1
        fi
    fi
    
    # V√©rifier curl pour les tests de connectivit√©
    if ! command -v curl &> /dev/null; then
        log_error "curl n'est pas install√©"
        exit 1
    fi
    
    log_success "Pr√©requis valid√©s"
}

# Tester la connectivit√© aux services
test_connectivity() {
    log_info "Test de connectivit√© aux services..."
    
    local services=("auth" "wallet")
    local all_healthy=true
    
    for service in "${services[@]}"; do
        local url="${BASE_URL}/api/${service}/actuator/health"
        
        if curl -s -f "$url" > /dev/null; then
            log_success "‚úì Service $service accessible"
        else
            log_warning "‚ö† Service $service non accessible: $url"
            all_healthy=false
        fi
    done
    
    if [[ "$all_healthy" != "true" ]]; then
        log_warning "Certains services ne sont pas accessibles"
        log_info "Les tests peuvent √©chouer si les services ne sont pas d√©marr√©s"
        
        read -p "Continuer quand m√™me ? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi
}

# Ex√©cuter les tests unitaires
run_unit_tests() {
    if [[ "$SKIP_UNIT_TESTS" == "true" ]]; then
        log_info "Tests unitaires ignor√©s"
        return 0
    fi
    
    log_info "=== Ex√©cution des tests unitaires ==="
    
    local services=("auth-service" "wallet-service")
    local overall_success=true
    
    for service in "${services[@]}"; do
        log_info "Tests unitaires pour $service..."
        
        cd "${PROJECT_ROOT}/services/$service"
        
        if mvn clean test -Dspring.profiles.active=test; then
            log_success "‚úì Tests unitaires $service r√©ussis"
        else
            log_error "‚úó Tests unitaires $service √©chou√©s"
            overall_success=false
        fi
        
        # G√©n√©rer le rapport de couverture
        if mvn jacoco:report; then
            log_info "Rapport de couverture g√©n√©r√©: target/site/jacoco/index.html"
        fi
        
        cd - > /dev/null
    done
    
    if [[ "$overall_success" == "true" ]]; then
        log_success "Tous les tests unitaires ont r√©ussi"
    else
        log_error "Certains tests unitaires ont √©chou√©"
        return 1
    fi
}

# Ex√©cuter les tests d'int√©gration
run_integration_tests() {
    if [[ "$SKIP_INTEGRATION_TESTS" == "true" ]]; then
        log_info "Tests d'int√©gration ignor√©s"
        return 0
    fi
    
    log_info "=== Ex√©cution des tests d'int√©gration ==="
    
    cd "${TESTS_DIR}/integration"
    
    # Configurer les variables d'environnement pour les tests
    export BASE_URL="$BASE_URL"
    export SPRING_PROFILES_ACTIVE="test"
    
    if mvn clean verify -Dspring.profiles.active=test; then
        log_success "‚úì Tests d'int√©gration r√©ussis"
    else
        log_error "‚úó Tests d'int√©gration √©chou√©s"
        cd - > /dev/null
        return 1
    fi
    
    cd - > /dev/null
}

# Ex√©cuter les tests de performance
run_performance_tests() {
    if [[ "$SKIP_PERFORMANCE_TESTS" == "true" ]]; then
        log_info "Tests de performance ignor√©s"
        return 0
    fi
    
    log_info "=== Ex√©cution des tests de performance ==="
    
    cd "${TESTS_DIR}/performance"
    
    # Cr√©er le r√©pertoire de r√©sultats
    local results_dir="results/$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$results_dir"
    
    # Variables d'environnement pour K6
    export BASE_URL="$BASE_URL"
    export ENVIRONMENT="$ENVIRONMENT"
    
    # Test de performance du service d'authentification
    log_info "Test de performance - Service d'authentification..."
    if k6 run --duration "$PERFORMANCE_DURATION" \
             --out json="${results_dir}/auth-results.json" \
             --out influxdb=http://localhost:8086/k6 \
             auth-load-test.js; then
        log_success "‚úì Test de performance auth r√©ussi"
    else
        log_error "‚úó Test de performance auth √©chou√©"
    fi
    
    # Pause entre les tests
    sleep 30
    
    # Test de performance du service portefeuille
    log_info "Test de performance - Service portefeuille..."
    if k6 run --duration "$PERFORMANCE_DURATION" \
             --out json="${results_dir}/wallet-results.json" \
             --out influxdb=http://localhost:8086/k6 \
             wallet-load-test.js; then
        log_success "‚úì Test de performance wallet r√©ussi"
    else
        log_error "‚úó Test de performance wallet √©chou√©"
    fi
    
    # G√©n√©rer un rapport de synth√®se
    generate_performance_report "$results_dir"
    
    cd - > /dev/null
}

# G√©n√©rer un rapport de performance
generate_performance_report() {
    local results_dir="$1"
    local report_file="${results_dir}/performance-report.html"
    
    log_info "G√©n√©ration du rapport de performance..."
    
    cat > "$report_file" << EOF
<!DOCTYPE html>
<html>
<head>
    <title>Rapport de Performance ISMAIL - $(date)</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background: #f4f4f4; padding: 20px; border-radius: 5px; }
        .section { margin: 20px 0; }
        .metric { background: #e8f5e8; padding: 10px; margin: 5px 0; border-radius: 3px; }
        .error { background: #ffe8e8; }
        .warning { background: #fff8e8; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Rapport de Performance ISMAIL</h1>
        <p>Date: $(date)</p>
        <p>Environnement: $ENVIRONMENT</p>
        <p>URL de base: $BASE_URL</p>
        <p>Dur√©e des tests: $PERFORMANCE_DURATION</p>
    </div>
    
    <div class="section">
        <h2>R√©sultats des Tests</h2>
        <div class="metric">
            <strong>Service d'Authentification:</strong>
            <ul>
                <li>Fichier de r√©sultats: auth-results.json</li>
                <li>M√©triques disponibles dans Grafana</li>
            </ul>
        </div>
        <div class="metric">
            <strong>Service Portefeuille:</strong>
            <ul>
                <li>Fichier de r√©sultats: wallet-results.json</li>
                <li>M√©triques disponibles dans Grafana</li>
            </ul>
        </div>
    </div>
    
    <div class="section">
        <h2>Analyse des R√©sultats</h2>
        <p>Pour analyser les r√©sultats d√©taill√©s :</p>
        <ol>
            <li>Consultez les fichiers JSON dans ce r√©pertoire</li>
            <li>Visualisez les m√©triques dans Grafana</li>
            <li>V√©rifiez les seuils de performance d√©finis</li>
        </ol>
    </div>
</body>
</html>
EOF
    
    log_success "Rapport g√©n√©r√©: $report_file"
}

# G√©n√©rer un rapport de synth√®se global
generate_summary_report() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local report_file="${TESTS_DIR}/reports/test-summary-${timestamp}.md"
    
    mkdir -p "${TESTS_DIR}/reports"
    
    cat > "$report_file" << EOF
# Rapport de Tests ISMAIL - $(date)

## Configuration
- **Environnement**: $ENVIRONMENT
- **URL de base**: $BASE_URL
- **Dur√©e tests de performance**: $PERFORMANCE_DURATION

## R√©sultats

### Tests Unitaires
$(if [[ "$SKIP_UNIT_TESTS" == "true" ]]; then echo "- ‚è≠Ô∏è Ignor√©s"; else echo "- ‚úÖ Ex√©cut√©s"; fi)

### Tests d'Int√©gration
$(if [[ "$SKIP_INTEGRATION_TESTS" == "true" ]]; then echo "- ‚è≠Ô∏è Ignor√©s"; else echo "- ‚úÖ Ex√©cut√©s"; fi)

### Tests de Performance
$(if [[ "$SKIP_PERFORMANCE_TESTS" == "true" ]]; then echo "- ‚è≠Ô∏è Ignor√©s"; else echo "- ‚úÖ Ex√©cut√©s"; fi)

## Recommandations

1. Consultez les rapports d√©taill√©s dans chaque r√©pertoire de test
2. V√©rifiez les m√©triques dans Grafana pour les tests de performance
3. Analysez la couverture de code avec JaCoCo
4. Surveillez les alertes dans AlertManager

## Fichiers G√©n√©r√©s

- Rapports JaCoCo: \`services/*/target/site/jacoco/\`
- R√©sultats K6: \`tests/performance/results/\`
- Logs d√©taill√©s: \`tests/logs/\`

EOF
    
    log_success "Rapport de synth√®se g√©n√©r√©: $report_file"
}

# Fonction principale
main() {
    log_info "=== Ex√©cution des Tests ISMAIL ==="
    log_info "Environnement: $ENVIRONMENT"
    log_info "URL de base: $BASE_URL"
    echo
    
    check_prerequisites
    test_connectivity
    
    local start_time=$(date +%s)
    local overall_success=true
    
    # Ex√©cuter les diff√©rents types de tests
    if ! run_unit_tests; then
        overall_success=false
    fi
    
    if ! run_integration_tests; then
        overall_success=false
    fi
    
    if ! run_performance_tests; then
        overall_success=false
    fi
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    # G√©n√©rer le rapport de synth√®se
    generate_summary_report
    
    # R√©sum√© final
    echo
    log_info "=== R√©sum√© de l'Ex√©cution ==="
    log_info "Dur√©e totale: ${duration}s"
    
    if [[ "$overall_success" == "true" ]]; then
        log_success "üéâ Tous les tests ont r√©ussi !"
        exit 0
    else
        log_error "‚ùå Certains tests ont √©chou√©"
        exit 1
    fi
}

# Point d'entr√©e
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    parse_arguments "$@"
    main
fi
